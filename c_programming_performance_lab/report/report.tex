\documentclass[12pt,a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage{ucs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[english]{babel}
\usepackage{svg}	
\usepackage{graphicx}

\author{Jan-Hendrik Niemann and Andreas Radke}
\title{Parallel Programming Lab Assignment: Diffusion Algorithm}


\begin{document}
	\maketitle
	
\section{Operations and Complexity}\label{chp:opcom}

We chose the Diffusion Algorithm for our lab assignment.
The main part of the algorithm is determined by the loops. We have a triple nested loop in \emph{sum\_values} (called twice) for summing all the values of the 3D-matrix, a triple nested loop in \emph{init} for setting the initial values and the diffusion function with a four-fold nested loop which is the main part of the computation. Allocating the memory for the array with malloc is in regard to the other functions rather negligible. 

Therefore we get for a 3D-matrix with the size of $ N_x\times N_y \times N_z $ the number of operations: \[ 2 N_x N_y N_z + N_x N_y N_z + T N_x N_y N_z \]
where $ T =  \frac{N_x N_y N_z}{1000}$ (in the code it is named \emph{count}).
The first two summands are for bigger input sizes way smaller and hence can be neglected. This leads us to the complexity of $ \mathcal{O}(N_x^2 N_y^2 N_z^2) $.

\section{Base Version's Performance and Correctness}

As we can see in Figure \ref{fig:oper} the baseline version is very slow. For an input size of $N=200$ the improved version is nearly 12.6 times faster. It can handle $569\cdot10^6$ operations per second. The baseline version scores only $45\cdot10^6$ operations per second.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 2/oper"}
	\caption{Million Operations per Second}
	\label{fig:oper}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 2/clock"}
	\caption{Task-Clock (msec)}
	\label{fig:clock}
\end{figure}


\section{Effect of Problem Size and Variable Type}

As derived in Section \ref{chp:opcom} we can see in Figure \ref{fig:clock} that the computation time indeed grows exponentially fast. Note that the y-axis has a logarithmic scale. We can see as well that the baseline code is slower than the improved one. It is independent of the input size.

Later we will optimize different stages (initialization, diffusion and error computation (\emph{sum\_value}) at which we will see that the initialization has no real impact on the performance (for slightly bigger input values). 

Obviously the \emph{float} version of our program fares better in terms of performance (compare Figure \ref{fig:floatvsdouble}) than with \emph{double}. Instructions per cycle and operations per second are around 10 to 20\% better and for cycles, instructions, task-clock and execution time it takes only 80\% of the \emph{double} version. But the most glaring distinction is the decreased number of cache-misses (especially for higher input numbers) of the \emph{float} program. With this one can clearly see where the smaller data types come in handy. 
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 3/floatvsdouble"}
	\caption{Comparison of float vs double compilation. 100\% = double. }
	\label{fig:floatvsdouble}
\end{figure}



In Figure \ref{fig:oper} we can see the operations per second in [$N^6/s$]. Note that the baseline code slows down for higher input size. In contrast the improved version "jumps" when doubling the input size from $N=50$ to $N=100$. There are almost no differences for higher input sizes. A small variation can be seen for $N=150$.

\section{Compilers}

There is a slight difference between the GCC and the ICC compiler. However, in general there cannot be made a statement whether GCC or ICC is better. In Figure \ref{fig:instrcycle} we can see that ICC reaches a higher value for instructions per cycle than GCC. If we take a look at the improved version, we see that the GCC compiler reaches a higher value. The result looks similar if we compare values like instructions, cycles or time.

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark/Instrcycle"}
	\caption{Instructions per cycle}
	\label{fig:instrcycle}
\end{figure}

In Figure \ref{fig:Ocompelapsedtime} and \ref{fig:noopssec} one can see that the flag options bring a lot of performance improvements. The biggest jump is attained when activating \emph{-O1} (\emph{Diffg\_0O1}). It can be further improved with \emph{-O2} (\emph{Diffg\_0O2}) but between \emph{-O2}, \emph{-O3} and \emph{-OFast} there is almost no further improvement. 

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 3/elapsedtimepercOcomp"}
	\caption{Relation of elapsed Time of different Optimization Flags compared to no Optimization at all.}
	\label{fig:Ocompelapsedtime}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 3/noopssec"}
	\caption{Development of number of Operations per second for several flag options.  }
	\label{fig:noopssec}
\end{figure}

\section{Optimized Version without SIMD}

The most important part for our non-SIMD optimization was to change the order of the loops in the different functions. We started with reordering the \emph{init} method (\emph{Diff\_1}), followed by \emph{sum\_values} (\emph{Diff\_2}) and finally the \emph{diffusion} part (\emph{Diff\_3}). Additionally restructured some \emph{if}-conditions or variable assignments to outer loops because they had no dependence on the inner loop variables.  

 
\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark/seconds"}
	\caption{Elapsed Time in Seconds for the different Code Versions}
	\label{fig:seconds}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark/percentage"}
	\caption{Relative part of Cycles, Instructions, Cache-misses, task-clock and Seconds for the non-SIMD-optimized \emph{Diff\_3} and the SIMD-\emph{Diff\_5}. The percentages are in relation to the base program.}
	\label{fig:percentage}
\end{figure}

One can see in Figure \ref{fig:seconds} that the first two optimizations had barely an effect. But the change of the diffusion method with its four-fold loop leads to a execution time of roughly 10-12 \% compared to the base program. 

Although the number of instructions has not decreased vastly (around 83-85 \%, compare Figure \ref{fig:percentage}) in every other aspect \emph{Diff\_3} performs well better than \emph{Diff\_0} with only 10-11\% cycles, task-clock and elapsed time and even less than 1\% in cache-misses. 

\section{Optimized Version with SIMD}

For the vectorization we had to adapt the \emph{diffusion} method a bit more. Because of the specific boundary conditions the most inner loop could only go from $ 1 < z < N_z - 1 $. Therefore computed the two boundary points separately before and after the loop. \emph{Diff\_4} contains the directive only for the inner \emph{diffusion} loop whereas \emph{diff\_5} has additionally active SIMD directives for \emph{init} and \emph{sum\_values}. Again one can see (compare Figure \ref{fig:percentage}) that the latter two functions have an insignificant role. But overall the SIMD instruction is a big improvement again. Especially the relative part of instructions (compared to \emph{Diff\_0}) shrunk from 84-85\% to 19-25\%. Equally the stats in cycles, task-clock and elapsed time improved further. 

\section{Performance Bottlenecks}
\end{document}
