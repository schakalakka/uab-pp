\documentclass[12pt,a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage{ucs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[english]{babel}
\usepackage{svg}	
\usepackage{graphicx}

\author{Jan-Hendrik Niemann and Andreas Radke}
\title{Parallel Programming Lab Assignment: Diffusion Algorithm}


\begin{document}
	\maketitle
	
	
\section*{Glossary} 
For easier understanding we want to put a short explanation for each compiled program and its used flags. 

Diff\textcolor{green}{x}\_\textcolor{blue}{n}\textcolor{red}{\{suffix\}}
\begin{itemize}
\item \textcolor{green}{x} $ \in \{g,i\} $ - determines the used compiler, gcc or icc,
\item \textcolor{blue}{n} $ \in \{0,1,2,3,4,5\} $ - grade of optimization (in the written code, not respective to the flag): 0 - given base program, 1, 2, 3 - loop reordering (see Section \ref{chp:optwosimd}), 4, 5 - use vector/SIMD instructions (Section \ref{chp:optwsimd}),
\item \textcolor{red}{\{suffix\}} - possible suffixes \emph{O1, O2, O3} (for the compiler flags -O1 etc.) or \emph{Oslow} (for no optimization at all), \emph{Double} for a compiled version of \emph{REAL double} or none at all (if the optimization flag is not stated always Ofast is used).
\end{itemize}

Examples:
\begin{itemize}
\item Diffi\_3 - full loop reordering, compiled with icc -Ofast diff.c -o Diffi\_3  
\item Diffi\_5 - full SIMD instructions, compiled with icc -Ofast -openmp diff.c -o Diffi\_5 
\item Diffg\_0O2 - base program with -O2 flag, compiled with gcc -O2 diff.c -o Diffg\_0O2
\item Diffg\_0Double - REAL is equal to double instead of float, compiled with gcc -Ofast diff.c -o Diffg\_0Double
\end{itemize} 
 

\section{Operations and Complexity}\label{chp:opcom}

We chose the Diffusion Algorithm for our lab assignment.
The main part of the algorithm is determined by the loops. We have a triple nested loop in \emph{sum\_values} (called twice) for summing all the values of the 3D-matrix, a triple nested loop in \emph{init} for setting the initial values and the diffusion function with a four-fold nested loop which is the main part of the computation. Allocating the memory for the array with malloc is in regard to the other functions rather negligible. 

Therefore we get for a 3D-matrix with the size of $ N_x\times N_y \times N_z $ the number of operations: \[ 2 N_x N_y N_z + N_x N_y N_z + T N_x N_y N_z \]
where $ T =  \frac{N_x N_y N_z}{1000}$ (in the code it is named \emph{count}).
The first two summands are for bigger input sizes way smaller and hence can be neglected. This leads us to the complexity of $ \mathcal{O}(N_x^2 N_y^2 N_z^2) $.

\section{Base Version's Performance}

As we can see in Figure \ref{fig:oper} the baseline version is very slow. For an input size of $N=200$ the improved version is nearly 12.6 times faster. It can handle $569\cdot10^6$ operations per second. The baseline version scores only $45\cdot10^6$ operations per second.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 2/oper"}
	\caption{Million Operations per Second}
	\label{fig:oper}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 2/instructionsinput"}
	\caption{Number of Instructions}
	\label{fig:instructionsinput}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 2/growth"}
	\caption{Growth of some Performance Stats for the Base Program. }
	\label{fig:growth}
\end{figure}


\section{Effect of Problem Size and Variable Type}

As derived in Section \ref{chp:opcom} we can see in Figure \ref{fig:instructionsinput} and \ref{fig:growth} that the number of instructions indeed seems to grow polynomially with an order of 6. For the other stats it shows mostly the same picture. We can see as well that the baseline code performs way worse than the fully optimized version. 

Later we will optimize different stages (initialization, diffusion and error computation (\emph{sum\_value}) at which we will see that the initialization has no real impact on the performance (for slightly bigger input values). 

Obviously the \emph{float} version of our program fares better in terms of performance (compare Figure \ref{fig:floatvsdouble}) than with \emph{double}. Instructions per cycle and operations per second are around 10 to 20\% better and for cycles, instructions, task-clock and execution time it takes only 80\% of the \emph{double} version. But the most glaring distinction is the decreased number of cache-misses (especially for higher input numbers) of the \emph{float} program. With this one can clearly see where the smaller data types come in handy. 
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 3/floatvsdouble"}
	\caption{Comparison of float vs double compilation. 100\% = double. }
	\label{fig:floatvsdouble}
\end{figure}



In Figure \ref{fig:oper} we can see the operations per second in [$N^6/s$]. Note that the baseline code slows down for higher input size. In contrast the improved version "jumps" when doubling the input size from $N=50$ to $N=100$. There are almost no differences for higher input sizes. A small variation can be seen for $N=150$.

\section{Compilers}

There is a only slight difference between the GCC and the ICC compiler. However, in general there cannot be made a statement whether GCC or ICC is better. In Figure \ref{fig:instrcycle} we can see that ICC reaches a higher value for instructions per cycle than GCC. If we take a look at the improved version, we see that the GCC compiler reaches a higher value. The result looks similar if we compare values like instructions, cycles or time.

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark/Instrcycle"}
	\caption{Instructions per cycle}
	\label{fig:instrcycle}
\end{figure}

In Figure \ref{fig:Ocompelapsedtime} and \ref{fig:noopssec} one can see that the flag options bring a lot of performance improvements. The biggest jump is attained when activating \emph{-O1} (\emph{Diffg\_0O1}). It can be further improved with \emph{-O2} (\emph{Diffg\_0O2}) but between \emph{-O2}, \emph{-O3} and \emph{-OFast} there is almost no further improvement. 

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 3/elapsedtimepercOcomp"}
	\caption{Relation of elapsed Time of different Optimization Flags compared to no Optimization at all.}
	\label{fig:Ocompelapsedtime}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 3/noopssec"}
	\caption{Development of number of Operations per second for several flag options.  }
	\label{fig:noopssec}
\end{figure}

\section{Optimized Version without SIMD}\label{chp:optwosimd}

The most important part for our non-SIMD optimization was to change the order of the loops in the different functions. We started with reordering the \emph{init} method (\emph{Diff\_1}), followed by \emph{sum\_values} (\emph{Diff\_2}) and finally the \emph{diffusion} part (\emph{Diff\_3}). Additionally restructured some \emph{if}-conditions or variable assignments to outer loops because they had no dependence on the inner loop variables.  

 
\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark/seconds"}
	\caption{Elapsed Time in Seconds for the different Code Versions}
	\label{fig:seconds}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark/percentage"}
	\caption{Relative part of Cycles, Instructions, Cache-misses, task-clock and Seconds for the non-SIMD-optimized \emph{Diff\_3} and the SIMD-\emph{Diff\_5}. The percentages are in relation to the base program.}
	\label{fig:percentage}
\end{figure}

One can see in Figure \ref{fig:seconds} that the first two optimizations had barely an effect. But the change of the diffusion method with its four-fold loop leads to a execution time of roughly 10-12 \% compared to the base program. 

Although the number of instructions has not decreased vastly (around 83-85 \%, compare Figure \ref{fig:percentage}) in every other aspect \emph{Diff\_3} performs well better than \emph{Diff\_0} with only 10-11\% cycles, task-clock and elapsed time and even less than 1\% in cache-misses. 

\section{Optimized Version with SIMD}\label{chp:optwsimd}

For the vectorization we had to adapt the \emph{diffusion} method a bit more. Because of the specific boundary conditions the most inner loop could only go from $ 1 < z < N_z - 1 $. Therefore computed the two boundary points separately before and after the loop. \emph{Diff\_4} contains the directive only for the inner \emph{diffusion} loop whereas \emph{diff\_5} has additionally active SIMD directives for \emph{init} and \emph{sum\_values}. Again one can see (compare Figure \ref{fig:percentage}) that the latter two functions have an insignificant role. But overall the SIMD instruction is a big improvement again. Especially the relative part of instructions (compared to \emph{Diff\_0}) shrunk from 84-85\% to 19-25\%. Equally the stats in cycles, task-clock and elapsed time improved further. 

For the compilation the flags -fopenmp and -openmp respectively were added. 

\section{Performance Bottlenecks}

In Figure \ref{fig:growthoptimized} are the performance stats of the optimized version (\emph{Diffg\_5}) shown for varying input sizes. As the base of 100\% the input matrix of size $ 50\times 50 \times 50 $ is used. One can see that the number of instructions, the cycles and the task-clock behave quite similar. But the real problem here is the growth of cache-misses. One can conclude from this data that for increasing 3D-matrix sizes the CPU's cache does not suffices the program's memory consumption and hence has to use the RAM.  


\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Benchmark 2/growthoptimized"}
	\caption{Growth of some Performance Stats for the optimized Program. }
	\label{fig:growthoptimized}
\end{figure}

\end{document}
